{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksJVcTLxzskm"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N8fPchWjlyM1"
      },
      "outputs": [],
      "source": [
        "import qiskit\n",
        "import pylatexenc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BoQlEyV_jY1G"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from functools import reduce\n",
        "from collections import deque, defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GfudJfnDmH06"
      },
      "outputs": [],
      "source": [
        "import qiskit\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math, random, time, itertools\n",
        "from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister\n",
        "from qiskit.quantum_info.operators import Operator\n",
        "from qiskit.circuit import Parameter, ParameterVector\n",
        "from tqdm.notebook import tqdm\n",
        "from qiskit.opflow import Z, X, I, StateFn, CircuitStateFn, SummedOp, CircuitOp\n",
        "from qiskit.opflow.gradients import Gradient, NaturalGradient, QFI, Hessian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jySdpQf_zuiE"
      },
      "source": [
        "# Build CIrucit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qSOTIhg1jY1H"
      },
      "outputs": [],
      "source": [
        "def add_single_qubit_rotation(circuit, Paramvector, n_qubits):\n",
        "    for i in range(n_qubits):\n",
        "        circuit.rx(Paramvector[i], i)\n",
        "        circuit.ry(Paramvector[i], i)\n",
        "        circuit.rz(Paramvector[i], i)\n",
        "def add_encoding_layer(circuit, Paramvector, n_qubits):\n",
        "    for i in range(n_qubits):\n",
        "        circuit.rx(Paramvector[i], i)\n",
        "def add_entangling_layer(circuit, n_qubits):\n",
        "    qubits = [i for i in range(n_qubits)]\n",
        "    for c in itertools.combinations(qubits, 2):\n",
        "        circuit.cz(c[0], c[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Lw-FrrkkjY1I"
      },
      "outputs": [],
      "source": [
        "def build_circuit(n_qubits, n_layers):\n",
        "    qr = QuantumRegister(n_qubits)\n",
        "    qc = QuantumCircuit(qr)\n",
        "    \n",
        "    param_rot = ParameterVector('Rot', 3*n_qubits*(n_layers+1))\n",
        "    param_enc = ParameterVector('Enc', n_qubits*n_layers)\n",
        "    \n",
        "    for l in range(n_layers):\n",
        "        # Variational + Encoding Layer\n",
        "        add_single_qubit_rotation(qc, param_rot[l*(3*n_qubits):(l+1)*(3*n_qubits)], n_qubits)\n",
        "        qc.barrier()\n",
        "        add_entangling_layer(qc, n_qubits)\n",
        "        qc.barrier()\n",
        "        # Encoding Layer\n",
        "        add_encoding_layer(qc, param_enc[l*n_qubits:(l+1)*n_qubits], n_qubits)\n",
        "        qc.barrier()\n",
        "    # Last Variational Layer\n",
        "    add_single_qubit_rotation(qc, param_rot[n_layers*(3*n_qubits):(n_layers+1)*(3*n_qubits)], n_qubits)\n",
        "    \n",
        "    return qc, param_rot, param_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "wJWfxOCKjY1I",
        "outputId": "1f5bb2a0-2c03-4857-c193-873d3143d1b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">      ┌────────────┐┌────────────┐┌────────────┐ ░           ░ ┌────────────┐»\n",
              "q2_0: ┤ Rx(Rot[0]) ├┤ Ry(Rot[0]) ├┤ Rz(Rot[0]) ├─░──■──■─────░─┤ Rx(Enc[0]) ├»\n",
              "      ├────────────┤├────────────┤├────────────┤ ░  │  │     ░ ├────────────┤»\n",
              "q2_1: ┤ Rx(Rot[1]) ├┤ Ry(Rot[1]) ├┤ Rz(Rot[1]) ├─░──■──┼──■──░─┤ Rx(Enc[1]) ├»\n",
              "      ├────────────┤├────────────┤├────────────┤ ░     │  │  ░ ├────────────┤»\n",
              "q2_2: ┤ Rx(Rot[2]) ├┤ Ry(Rot[2]) ├┤ Rz(Rot[2]) ├─░─────■──■──░─┤ Rx(Enc[2]) ├»\n",
              "      └────────────┘└────────────┘└────────────┘ ░           ░ └────────────┘»\n",
              "«       ░  ┌────────────┐ ┌────────────┐ ┌────────────┐\n",
              "«q2_0: ─░──┤ Rx(Rot[9]) ├─┤ Ry(Rot[9]) ├─┤ Rz(Rot[9]) ├\n",
              "«       ░ ┌┴────────────┤┌┴────────────┤┌┴────────────┤\n",
              "«q2_1: ─░─┤ Rx(Rot[10]) ├┤ Ry(Rot[10]) ├┤ Rz(Rot[10]) ├\n",
              "«       ░ ├─────────────┤├─────────────┤├─────────────┤\n",
              "«q2_2: ─░─┤ Rx(Rot[11]) ├┤ Ry(Rot[11]) ├┤ Rz(Rot[11]) ├\n",
              "«       ░ └─────────────┘└─────────────┘└─────────────┘</pre>"
            ],
            "text/plain": [
              "      ┌────────────┐┌────────────┐┌────────────┐ ░           ░ ┌────────────┐»\n",
              "q2_0: ┤ Rx(Rot[0]) ├┤ Ry(Rot[0]) ├┤ Rz(Rot[0]) ├─░──■──■─────░─┤ Rx(Enc[0]) ├»\n",
              "      ├────────────┤├────────────┤├────────────┤ ░  │  │     ░ ├────────────┤»\n",
              "q2_1: ┤ Rx(Rot[1]) ├┤ Ry(Rot[1]) ├┤ Rz(Rot[1]) ├─░──■──┼──■──░─┤ Rx(Enc[1]) ├»\n",
              "      ├────────────┤├────────────┤├────────────┤ ░     │  │  ░ ├────────────┤»\n",
              "q2_2: ┤ Rx(Rot[2]) ├┤ Ry(Rot[2]) ├┤ Rz(Rot[2]) ├─░─────■──■──░─┤ Rx(Enc[2]) ├»\n",
              "      └────────────┘└────────────┘└────────────┘ ░           ░ └────────────┘»\n",
              "«       ░  ┌────────────┐ ┌────────────┐ ┌────────────┐\n",
              "«q2_0: ─░──┤ Rx(Rot[9]) ├─┤ Ry(Rot[9]) ├─┤ Rz(Rot[9]) ├\n",
              "«       ░ ┌┴────────────┤┌┴────────────┤┌┴────────────┤\n",
              "«q2_1: ─░─┤ Rx(Rot[10]) ├┤ Ry(Rot[10]) ├┤ Rz(Rot[10]) ├\n",
              "«       ░ ├─────────────┤├─────────────┤├─────────────┤\n",
              "«q2_2: ─░─┤ Rx(Rot[11]) ├┤ Ry(Rot[11]) ├┤ Rz(Rot[11]) ├\n",
              "«       ░ └─────────────┘└─────────────┘└─────────────┘"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "def draw_example():\n",
        "    quantum_circuit, param_rot, param_enc = build_circuit(n_qubits=3, n_layers=1)\n",
        "    return quantum_circuit\n",
        "draw_example().draw()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dAGK-IntjY1J"
      },
      "outputs": [],
      "source": [
        "class PQC_with_DataReuploading(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers, output_dim, observables=None, activation='linear'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_layers = n_layers\n",
        "        self.output_dim = output_dim\n",
        "        self.activation = activation\n",
        "        \n",
        "        if observables == None:\n",
        "            self.observables = Z^n_qubits\n",
        "        else:\n",
        "            self.observables = observables\n",
        "        \n",
        "        # Build circuits / Parameter Vectors\n",
        "        self.circuit, self.rot_params, self.enc_params = build_circuit(self.n_qubits, self.n_layers)\n",
        "        self.len_rot_params = len(self.rot_params)\n",
        "        self.len_enc_params = len(self.enc_params)\n",
        "        \n",
        "        self.total_params = []\n",
        "        for p in self.rot_params:\n",
        "            self.total_params.append(p)\n",
        "        for p in self.enc_params:\n",
        "            self.total_params.append(p)\n",
        "        \n",
        "        # Initial Parameters for circuit\n",
        "        self.rot_param_vals = nn.Parameter(np.pi * torch.rand(len(self.rot_params)))\n",
        "        self.enc_param_vals = nn.Parameter(torch.ones(len(self.enc_params)))\n",
        "        \n",
        "        # Parameter for circuit output\n",
        "        self.w = nn.Parameter(nn.Parameter(torch.rand(self.output_dim)))\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Input: State values -> Need to insert into Encoding gates with scaling parameters(encoding params)\n",
        "        input_tiled = inputs.tile(self.n_layers)\n",
        "#         print(input_tiled)\n",
        "        input_scaled = self.enc_param_vals * input_tiled\n",
        "        input_params = input_scaled\n",
        "    \n",
        "        \n",
        "        # Make state fucntion & Get expectation value of observable\n",
        "        circ_psi = CircuitStateFn(primitive=self.circuit, coeff=1.)\n",
        "        expectation = ~ circ_psi @ self.observables @ circ_psi\n",
        "        \n",
        "        # Assignn Parameter values\n",
        "        value_dict = {}\n",
        "        for i, p in enumerate(self.rot_params):\n",
        "            value_dict[p] = self.rot_param_vals[i].item()\n",
        "        for i, p in enumerate(self.enc_params):\n",
        "            value_dict[p] = input_params[i].detach().item()\n",
        "            \n",
        "        # Final expectation value\n",
        "        expectation_val = expectation.assign_parameters(value_dict).eval().real\n",
        "        action_exp = torch.tensor(expectation_val) * self.w\n",
        "        action_prob = F.softmax(action_exp, dim=0)\n",
        "        \n",
        "        return action_prob\n",
        "    \n",
        "    def get_grad(self, inputs):\n",
        "        input_tiled = inputs.tile(self.n_layers)\n",
        "        input_scaled = self.enc_param_vals * input_tiled\n",
        "        input_params = input_scaled\n",
        "        \n",
        "        value_dict = {}\n",
        "        for i, p in enumerate(self.rot_params):\n",
        "            value_dict[p] = self.rot_param_vals[i].item()\n",
        "        for i, p in enumerate(self.enc_params):\n",
        "            value_dict[p] = input_params[i].detach().item()\n",
        "        \n",
        "        psi = CircuitStateFn(primitive=self.circuit, coeff=1.)\n",
        "        op = ~StateFn(self.observables) @ psi\n",
        "        grad = Gradient(grad_method='param_shift').convert(operator=op, params=self.total_params)\n",
        "        grad_val = grad.assign_parameters(value_dict).eval()\n",
        "        rot_grad = torch.tensor([grad_val[i].real for i in range(self.len_rot_params)])\n",
        "        enc_grad = torch.tensor([grad_val[self.len_rot_params + i].real for i in range(self.len_enc_params)])\n",
        "        \n",
        "        expectation = ~ psi @ self.observables @ psi\n",
        "        expectation_val = expectation.assign_parameters(value_dict).eval().real\n",
        "        w_grad = torch.tensor([float(expectation_val) for i in range(len(self.w))])\n",
        "        return rot_grad, enc_grad, w_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "p5VE6baRjY1K"
      },
      "outputs": [],
      "source": [
        "policy = PQC_with_DataReuploading(n_qubits=4, n_layers=1, output_dim=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxRegfDLjY1K"
      },
      "outputs": [],
      "source": [
        "policy.circuit.draw()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-0Jc3t7TjY1L"
      },
      "outputs": [],
      "source": [
        "class QuantumAgent():\n",
        "    def __init__(self, input_state_dim, n_actions, n_layers=1):\n",
        "        self.policy = PQC_with_DataReuploading(n_qubits=input_state_dim, n_layers=n_layers, \n",
        "                                               output_dim=n_actions, observables=None,\n",
        "                                               activation='linear')\n",
        "        self.variational_optim = torch.optim.Adam([self.policy.rot_param_vals])\n",
        "        self.encoding_optim = torch.optim.Adam([self.policy.enc_param_vals])\n",
        "        self.weight_optim = torch.optim.Adam([self.policy.w])\n",
        "        \n",
        "        self.optims = [self.variational_optim, self.encoding_optim, self.weight_optim]\n",
        "        \n",
        "    def get_actions(self, input_state):\n",
        "        return self.policy.forward(input_state)\n",
        "    \n",
        "    def update_policy(self, states, id_action_pairs, returns, action_probs, batch_size):\n",
        "        rot_gradient, enc_gradient, w_gradient = [], [], []\n",
        "        for s in states:\n",
        "            r_grad, e_grad, w_grad = self.policy.get_grad(s)\n",
        "            rot_gradient.append(np.array(r_grad))\n",
        "            enc_gradient.append(np.array(e_grad))\n",
        "            w_gradient.append(np.array(w_grad))\n",
        "        rot_gradient = torch.mean(torch.from_numpy(np.array(rot_gradient)), dim=0)\n",
        "        enc_gradient = torch.mean(torch.from_numpy(np.array(enc_gradient)), dim=0)\n",
        "        w_gradient = torch.mean(torch.from_numpy(np.array(w_gradient)), dim=0)\n",
        "        \n",
        "        p_actions = torch.tensor([action_probs[id_action_pairs[i][0], id_action_pairs[i][1]] for i in range(action_probs.shape[0])])\n",
        "        log_probs = torch.log(p_actions)\n",
        "        loss = torch.sum(-log_probs * returns) / batch_size\n",
        "        \n",
        "        for opt in self.optims:\n",
        "            opt.zero_grad()\n",
        "        \n",
        "        self.policy.rot_param_vals.grad = loss * rot_gradient\n",
        "        self.policy.enc_param_vals.grad = loss * enc_gradient\n",
        "        self.policy.w.grad = loss * w_gradient\n",
        "                \n",
        "        self.variational_optim.step()\n",
        "        self.encoding_optim.step()\n",
        "        self.weight_optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "J9x6VCfIjY1L"
      },
      "outputs": [],
      "source": [
        "def gather_episodes(state_bounds, n_actions, agent, n_episodes, env_name):\n",
        "    \"\"\"Interact with environment in batched fashion.\"\"\"\n",
        "\n",
        "    trajectories = [defaultdict(list) for _ in range(n_episodes)]\n",
        "    envs = [gym.make(env_name) for _ in range(n_episodes)]\n",
        "\n",
        "    done = [False for _ in range(n_episodes)]\n",
        "    states = [e.reset() for e in envs]\n",
        "\n",
        "    while not all(done):\n",
        "        unfinished_ids = [i for i in range(n_episodes) if not done[i]]\n",
        "        normalized_states = [s/state_bounds for i, s in enumerate(states) if not done[i]]\n",
        "\n",
        "        for i, state in zip(unfinished_ids, normalized_states):\n",
        "            trajectories[i]['states'].append(state)\n",
        "\n",
        "        states = torch.from_numpy(np.array(normalized_states))\n",
        "        action_probs = torch.from_numpy(np.array([agent.get_actions(states[i]).detach().numpy() for i in range(len(states))]))\n",
        "\n",
        "        # Store action and transition all environments to the next state\n",
        "        states = [None for i in range(n_episodes)]\n",
        "        for i, action_prob in zip(unfinished_ids, action_probs.numpy()):\n",
        "            action = np.random.choice(n_actions, p=action_prob)\n",
        "            states[i], reward, done[i], _ = envs[i].step(action)\n",
        "            trajectories[i]['actions'].append(action)\n",
        "            trajectories[i]['rewards'].append(reward)\n",
        "            trajectories[i]['action probs'].append(action_prob)\n",
        "\n",
        "    return trajectories\n",
        "\n",
        "def compute_returns(rewards_history, gamma):\n",
        "    \"\"\"Compute discounted returns with discount factor `gamma`.\"\"\"\n",
        "    returns = []\n",
        "    discounted_sum = 0\n",
        "    for r in rewards_history[::-1]:\n",
        "        discounted_sum = r + gamma * discounted_sum\n",
        "        returns.insert(0, discounted_sum)\n",
        "\n",
        "    # Normalize them for faster and more stable learning\n",
        "    returns = np.array(returns)\n",
        "    returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
        "    returns = returns.tolist()\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "kgPgfLlDjY1L"
      },
      "outputs": [],
      "source": [
        "env_name = \"CartPole-v1\"\n",
        "n_qubits = 4 # Dimension of the state vectors in CartPole\n",
        "n_layers = 1\n",
        "n_actions = 2 # Number of actions in CartPole\n",
        "state_bounds = np.array([2.4, 2.5, 0.21, 2.5])\n",
        "gamma = 1\n",
        "batch_size = 1\n",
        "n_episodes = 100\n",
        "\n",
        "agent = QuantumAgent(input_state_dim = n_qubits, n_actions=n_actions, n_layers=n_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0j1BTngjY1M",
        "outputId": "e6bc0273-ff05-42af-d410-54f0d59ed8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished episode 1 Average rewards:  16.0\n",
            "Finished episode 2 Average rewards:  15.0\n",
            "Finished episode 3 Average rewards:  17.0\n",
            "Finished episode 4 Average rewards:  23.0\n",
            "Finished episode 5 Average rewards:  18.0\n",
            "Finished episode 6 Average rewards:  18.0\n",
            "Finished episode 7 Average rewards:  35.0\n",
            "Finished episode 8 Average rewards:  15.0\n",
            "Finished episode 9 Average rewards:  12.0\n",
            "Finished episode 10 Average rewards:  37.0\n",
            "Finished episode 11 Average rewards:  12.0\n",
            "Finished episode 12 Average rewards:  17.0\n",
            "Finished episode 13 Average rewards:  11.0\n",
            "Finished episode 14 Average rewards:  11.0\n",
            "Finished episode 15 Average rewards:  15.0\n",
            "Finished episode 16 Average rewards:  18.0\n",
            "Finished episode 17 Average rewards:  20.0\n",
            "Finished episode 18 Average rewards:  52.0\n",
            "Finished episode 19 Average rewards:  32.0\n",
            "Finished episode 20 Average rewards:  11.0\n",
            "Finished episode 21 Average rewards:  13.0\n",
            "Finished episode 22 Average rewards:  31.0\n",
            "Finished episode 23 Average rewards:  23.0\n",
            "Finished episode 24 Average rewards:  17.0\n",
            "Finished episode 25 Average rewards:  18.0\n",
            "Finished episode 26 Average rewards:  22.0\n",
            "Finished episode 27 Average rewards:  13.0\n",
            "Finished episode 28 Average rewards:  15.0\n",
            "Finished episode 29 Average rewards:  9.0\n",
            "Finished episode 30 Average rewards:  13.0\n",
            "Finished episode 31 Average rewards:  16.0\n"
          ]
        }
      ],
      "source": [
        "# Start training the agent\n",
        "episode_reward_history = []\n",
        "for batch in range(n_episodes // batch_size):\n",
        "    # Gather episodes\n",
        "    episodes = gather_episodes(state_bounds, n_actions, agent, batch_size, env_name)\n",
        "\n",
        "    # Group states, actions and returns in numpy arrays\n",
        "    states = torch.from_numpy(np.concatenate([ep['states'] for ep in episodes]))\n",
        "    actions = torch.from_numpy(np.concatenate([ep['actions'] for ep in episodes]))\n",
        "    action_probs = torch.from_numpy(np.concatenate([ep['action probs'] for ep in episodes]))\n",
        "    rewards = [ep['rewards'] for ep in episodes]\n",
        "    returns = np.concatenate([compute_returns(ep_rwds, gamma) for ep_rwds in rewards])\n",
        "    returns = torch.from_numpy(np.array(returns, dtype=np.float32))\n",
        "\n",
        "    id_action_pairs = torch.from_numpy(np.array([[i, a] for i, a in enumerate(actions)]))\n",
        "\n",
        "    # Update model parameters.\n",
        "    agent.update_policy(states, id_action_pairs, returns, action_probs, batch_size)\n",
        "\n",
        "    # Store collected rewards\n",
        "    for ep_rwds in rewards:\n",
        "        episode_reward_history.append(np.sum(ep_rwds))\n",
        "\n",
        "    avg_rewards = np.mean(episode_reward_history[-1:])\n",
        "\n",
        "    print('Finished episode', (batch + 1) * batch_size,\n",
        "          'Average rewards: ', avg_rewards)\n",
        "\n",
        "    if avg_rewards >= 500.0:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Tm2c_gnzmkjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "QuantumRL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}